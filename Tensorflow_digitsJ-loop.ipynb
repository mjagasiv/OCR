{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Training accuracy: 0.9231\n",
      "\n",
      "Row  0 : [0][1][1][0][1][1][0][1][1][0][5][4][3][3][5][5][7][8][8][9]\n",
      "Row  1 : [1][1][1][1][0][0][0][1][0][1][3][3][5][4][3][7][6][8][3]*[7]\n",
      "Row  2 : [5][1][1][1][1][1][1][8][0][1][3][2][3][2][4][7]*[7][8][5]*[5]*\n",
      "Row  3 : [1][1][0][1][1][0][0][0][1][0][3][3][4][4][5][8][8][7][5][8]\n",
      "Row  4 : [0][1][0][1][0][1][0][1][5][3]*[4][5][3][4][5]*[7][5][8][6][9]\n",
      "Row  5 : [1][1][0][1][0][1][1][1][1][0][5][5][8]*[3][4][7][5][7][6][1]\n",
      "Row  6 : [1][0][1][0][1][0][1][0][1][0][8][1][1][4][2][8][8][6][8][7]\n",
      "Row  7 : [1][0][0][1][1][1][0][0][0][1][2][0][7][4][3][8][7][6][1][8]*\n",
      "Row  8 : [1][0][1][1][1][1][1][0][0][1][7]*[8][2][5][3][8][7][8][7][6]\n",
      "Row  9 : [1][1][1][1][5]*[0][0][0][1][1][4][4]*[5][5][3][6][1][5][3][7]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import cv2\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def getBestShift(img):\n",
    "    cy,cx = ndimage.measurements.center_of_mass(img)\n",
    "\n",
    "    rows,cols = img.shape\n",
    "    shiftx = np.round(cols/2.0-cx).astype(int)\n",
    "    shifty = np.round(rows/2.0-cy).astype(int)\n",
    "\n",
    "    return shiftx,shifty\n",
    "    \n",
    "def getBestShift(img):\n",
    "    cy,cx = ndimage.measurements.center_of_mass(img)\n",
    "\n",
    "    rows,cols = img.shape\n",
    "    shiftx = np.round(cols/2.0-cx).astype(int)\n",
    "    shifty = np.round(rows/2.0-cy).astype(int)\n",
    "\n",
    "    return shiftx,shifty\n",
    "\n",
    "def shift(img,sx,sy):\n",
    "    rows,cols = img.shape\n",
    "    M = np.float32([[1,0,sx],[0,1,sy]])\n",
    "    shifted = cv2.warpAffine(img,M,(cols,rows))\n",
    "    return shifted\n",
    "\n",
    "# create a MNIST_data folder with the MNIST dataset if necessary   \n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\"\"\"\n",
    "a placeholder for our image data:\n",
    "None stands for an unspecified number of images\n",
    "784 = 28*28 pixel\n",
    "\"\"\"\n",
    "x = tf.placeholder(\"float\", [None, 784])\n",
    "\n",
    "# we need our weights for our neural net\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "# and the biases\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "\"\"\"\n",
    "softmax provides a probability based output\n",
    "we need to multiply the image values x and the weights\n",
    "and add the biases\n",
    "(the normal procedure, explained in previous articles)\n",
    "\"\"\"\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "\"\"\"\n",
    "y_ will be filled with the real values\n",
    "which we want to train (digits 0-9)\n",
    "for an undefined number of images\n",
    "\"\"\"\n",
    "y_ = tf.placeholder(\"float\", [None,10])\n",
    "\n",
    "\"\"\"\n",
    "we use the cross_entropy function\n",
    "which we want to minimize to improve our model\n",
    "\"\"\"\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "\"\"\"\n",
    "use a learning rate of 0.01\n",
    "to minimize the cross_entropy error\n",
    "\"\"\"\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# initialize all variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# create a session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# use 1000 batches with a size of 100 each to train our net\n",
    "for i in range(10000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  # run the train_step function with the given image values (x) and the real output (y_)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "sess.run(train_step,feed_dict={x:newTrain, y_:6})\n",
    "\n",
    "\"\"\"\n",
    "Let's get the accuracy of our model:\n",
    "our model is correct if the index with the highest y value\n",
    "is the same as in the real digit vector\n",
    "The mean of the correct_prediction gives us the accuracy.\n",
    "We need to run the accuracy function\n",
    "with our test set (mnist.test)\n",
    "We use the keys \"images\" and \"labels\" for x and y_\n",
    "\"\"\"\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "print( \"\\nTraining accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n",
    "\n",
    "################ END OF TRAINING ###############################\n",
    "\n",
    "# create an array where we can store our 4 pictures\n",
    "images = np.zeros((1,784))\n",
    "# and the correct values\n",
    "correct_vals = np.zeros((1,10))\n",
    "\n",
    "# read the image\n",
    "origImage = cv2.imread(\"form4.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "for R in range(10):\n",
    "    print(\"\\nRow \",R, \":\",end=\" \")\n",
    "    for C in range(20):\n",
    "        # Crop the image to a single digit (should be in a loop to scan)\n",
    "        # X and Y are counter-intuitive (switched row vs col)\n",
    "        ##### Example 0:  Number 0\n",
    "        offsetx = 110+(R*48) \n",
    "        offsety = 131+(C*52) \n",
    "        gray = origImage[offsetx:offsetx+44, offsety:offsety+48]    \n",
    "\n",
    "        ##### Example 1:  Number 7 -- correct\n",
    "        #offsetx = 301                                    \n",
    "        #offsety = 911 \n",
    "        #gray = gray[offsetx:offsetx+50, offsety:offsety+50]    \n",
    "\n",
    "        ##### Example 2:  Number 2 -- correct\n",
    "        #offsetx = 351                                    \n",
    "        #offsety = 811 \n",
    "        #gray = gray[offsetx:offsetx+50, offsety:offsety+50]    \n",
    "\n",
    "        ##### Example 3:  Number 3 -- correct\n",
    "        #offsetx = 256                                    \n",
    "        #offsety = 653\n",
    "        #gray = gray[offsetx:offsetx+50, offsety:offsety+50]    \n",
    "\n",
    "        ##### Example 4:  Number 7 -- correct\n",
    "        #offsetx = 208                                    \n",
    "        #offsety = 963\n",
    "        #gray = gray[offsetx:offsetx+50, offsety:offsety+50]    \n",
    "\n",
    "        ##### Example 5:  Number 6 -- correct\n",
    "        #offsetx = 348                                    \n",
    "        #offsety = 1065\n",
    "        #gray = gray[offsetx:offsetx+50, offsety:offsety+50]    \n",
    "\n",
    "\n",
    "        ##################### Pre-process the image ##########################\n",
    "\n",
    "        # resize the images and invert it (black background)\n",
    "        gray = cv2.resize(255-gray, (28, 28))\n",
    "\n",
    "        (thresh, gray) = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "        while np.sum(gray[0]) == 0:\n",
    "            gray = gray[1:]\n",
    "\n",
    "        while np.sum(gray[:,0]) == 0:\n",
    "            gray = np.delete(gray,0,1)\n",
    "\n",
    "        while np.sum(gray[-1]) == 0:\n",
    "            gray = gray[:-1]\n",
    "\n",
    "        while np.sum(gray[:,-1]) == 0:\n",
    "            gray = np.delete(gray,-1,1)\n",
    "\n",
    "        rows,cols = gray.shape\n",
    "\n",
    "        if rows > cols:\n",
    "            factor = 20.0/rows\n",
    "            rows = 20\n",
    "            cols = int(round(cols*factor))\n",
    "            gray = cv2.resize(gray, (cols,rows))\n",
    "        else:\n",
    "            factor = 20.0/cols\n",
    "            cols = 20\n",
    "            rows = int(round(rows*factor))\n",
    "            gray = cv2.resize(gray, (cols, rows))\n",
    "\n",
    "        colsPadding = (int(math.ceil((28-cols)/2.0)),int(math.floor((28-cols)/2.0)))\n",
    "        rowsPadding = (int(math.ceil((28-rows)/2.0)),int(math.floor((28-rows)/2.0)))\n",
    "        gray = np.lib.pad(gray,(rowsPadding,colsPadding),'constant')\n",
    "\n",
    "        shiftx,shifty = getBestShift(gray)\n",
    "        shifted = shift(gray,shiftx,shifty)\n",
    "        gray = shifted\n",
    "\n",
    "        # save the processed images\n",
    "        cv2.imwrite(\"proc_image\"+str(R)+str(C)+\".png\", gray)\n",
    "\n",
    "        ################ DONE Processing image ################\n",
    "\n",
    "        #\"\"\"\n",
    "        #all images in the training set have an range from 0-1\n",
    "        #and not from 0-255 so we divide our flatten images\n",
    "        #(a one dimensional vector with our 784 pixels)\n",
    "        #to use the same 0-1 based range\n",
    "        #\"\"\"\n",
    "        flatten = gray.flatten() / 255.0\n",
    "        #\"\"\"\n",
    "        #we need to store the flatten image and generate\n",
    "        #the correct_vals array\n",
    "        #correct_val for the first digit (9) would be\n",
    "        #[0,0,0,0,0,0,0,0,0,1]\n",
    "        #\"\"\"\n",
    "        i=0\n",
    "        images[i] = flatten\n",
    "        correct_val = np.zeros((10))\n",
    "        #correct_val[2] = 1\n",
    "        correct_vals[i] = correct_val\n",
    "        i += 1\n",
    "\n",
    "        #\"\"\"\n",
    "        #the prediction will be an array with four values,\n",
    "        #which show the predicted number\n",
    "        #\"\"\"\n",
    "        prediction = tf.argmax(y,1)\n",
    "        #\"\"\"\n",
    "        #we want to run the prediction and the accuracy function\n",
    "        #using our generated arrays (images and correct_vals)\n",
    "        #\"\"\"\n",
    "        #print(\"Top predicted value: \", sess.run(prediction, feed_dict={x: images, y_: correct_vals}))\n",
    "        print(sess.run(prediction, feed_dict={x: images, y_: correct_vals}), end=\"\")\n",
    "        #print(sess.run(accuracy, feed_dict={x: images, y_: correct_vals}))\n",
    "\n",
    "        pred_val = tf.argmax(y, 1)\n",
    "        pred_softmax = tf.nn.softmax(y)\n",
    "        pred_top_5 = tf.nn.top_k(pred_softmax, k=5, sorted=True)\n",
    "        final_value_top_5, result_top_5 = sess.run([pred_val, pred_top_5], feed_dict={x: images, y_: correct_vals})\n",
    "\n",
    "        #print(\"Top Five Values: \", result_top_5[1])\n",
    "        #print(\"Top Five Weights: \", result_top_5[0])\n",
    "        #print(\"Top Weight: \", result_top_5[0][0][0])\n",
    "        #print(\"Next Weight: \", result_top_5[0][0][1])\n",
    "\n",
    "        if (result_top_5[0][0][1] >= (0.8*result_top_5[0][0][0])): \n",
    "            print(\"*\", end=\"\")\n",
    "            #print(\"Warning:  Top 2 values in close range!!\")\n",
    "            #print(\"Top Five Values: \", result_top_5[1])\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
